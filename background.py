import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import ConvexHull
from scipy.spatial.distance import cdist

T=20000                              # total timesteps in dump file

def file_conversion(filename): 
    """
    
    convert a lammps dump file into a numpy.array 
    
       Parameters: filename: string
                       the name of the dumpfile
    
       Returns: list with format file_ns[l][j,i] l:timestep j:particle i:coordinate 
    
    """ 
    with open(filename,'r') as data_file:
        n=0
        for line in data_file:
            n+=1
            data=line.split() 
            if n==4:
                num_p=int(data[0])
                break
                   
    a=[]
    with open(filename,'r') as data_file:
        for line in data_file:
            data=line.split()
            if len(data)==6:
                if not data[0]=='ITEM:':
                    for i in range(len(data)):
                        a.append(float(data[i]))
    data=np.array(a).reshape(((T+1)*num_p,6)) # no timestep split here.
    # now the data has columns 'ITEM:' and 'ATOMS', which are not useful.
    data=data[:,2:6].reshape((T+1,num_p,4))
    return data

            
def cluster_split(file_ns):
    """
    
    split the file generated by file_conversion into pieces for each cluster
    
       Parameters: file_ns: list
                       the no splitted file generated by file_conversion
                       
       Returns: list with format file[l][k][j,i] l:timestep k:cluster j:particle i:coordinate 
    
    """
    b=[]
    for l in range(T+1):
        b.append(np.split(file_ns[l][file_ns[l][:,-1].argsort()],np.cumsum(np.unique(file_ns[l][:,3],return_counts=True)[1])))
    return b


def cm(file_ns,file,tau_r):
    """
    
    calculate the centre of mass for each cluster after relaxation time tau_r
       
       Parameters: file_ns: list
                       the no splitted file generated by file_conversion
                       
                   file: list
                       the splitted file generated by cluster_split
                       
                   tau_r: float
                       the relaxation time
                 
       Return: list with format cm_file[l][k][i]. l:timestep k:cluster i:coordinate
    
    """
    cm_file=[]
    for l in range(tau_r,T+1):    
        at_l=[]
        cluster_data=np.unique(file_ns[l][:,3],return_counts=True)
        for k in range(len(cluster_data[0])):
            x_1=np.sum(file[l][k][:,0])/cluster_data[1][k];y_1=np.sum(file[l][k][:,1])/cluster_data[1][k];z_1=np.sum(file[l][k][:,2])/cluster_data[1][k]   
            at_l.append([x_1,y_1,z_1])
        cm_file.append(at_l)
    return cm_file    
    

def rgc(file_ns,file,cm_file,tau_r):
    """
    
    calculate the radius of gyration of cluster for each cluster
       
       Parameters: cm_file: list
                       the file generated by cm
       
       Return: list with format rgc_file[l][k]. l:timestep k:cluster
    
    """    
    a_3=[]
    for l in range(tau_r,T+1):    
        cluster_data=np.unique(file_ns[l][:,3],return_counts=True)
        a_2=[]
        for k in range(len(cluster_data[0])):
            num_p=cluster_data[1][k]
            a_1=0
            for j in range(num_p):
                for i in range(3):
                    a_1+=(cm_file[l][k][i]-file[l][k][j][i])**2/num_p # initial timestep, first cluster, x coordinate
            a_2.append(a_1)
        a_3.append(a_2)
    return a_3    
    

def gt(file_ns,file,cm_file,tau_r):
    """
    
    calculate the gyration tensor for each cluster
       
       Parameters: the same to rgc
       
       Return: list with format gt_file[l][k][i_1,i_2]. l:timestep k:cluster i_1,i_2:coordinate
       
    """
    a_4=[]
    N1=[];#number of particles for all timesteps
    for l in range(tau_r,T+1):    
        cluster_data=np.unique(file_ns[l][:,3],return_counts=True)
        a_3=[]
        N2=[];#number of particles for cluster k
        for k in range(len(cluster_data[0])):           
            num_p=cluster_data[1][k]
            a_2=[]
            for i_1 in range(3):
                for i_2 in range(3):
                    a_1=0
                    for j in range(num_p):
                        a_1+=(cm_file[l-tau_r][k][i_1]-file[l][k][j,i_1])*(cm_file[l-tau_r][k][i_2]-file[l][k][j,i_2])/num_p
                    a_2.append(a_1)
            a_2=np.array(a_2).reshape((3,3))
            a_3.append(a_2);N2.append(num_p)           
        a_4.append(a_3);N1.append(N2)
    return a_4,N1    
            

def mm(a,b): 
    """
    
    do a multiplication of 3 by 3 matrix and a 3 by 1 vector
    
       Parameters: a: ndarray
                    the 3 by 3 matrix
                   
                   b: ndarray
                    the 3 by 1 vector
       
       Return: 3 by 1 vector
    
    """
    a_1=[]
    for i in range(3):
        a_2=0
        for j in range(3):
            a_2+=(a[i,j]*b[j])
        a_1.append(a_2)
    return a_1


def e_v(gt_file):
    """
    
    calculate the eigenvalues and eigenvectors of gyration tensor for each time step
    
    Parameters
    ----------
    gt_file: the gyration tensor generated by the function gt

    Returns
    -------
    e_value: list with format e_value[t][c][i]. t:timestep c:cluster i:coordinate 
    e_vec: list with format e_vec[t][c][i_1,i_2]. t:timestep c:cluster i_1,i_2:coordinate

    """
    value_list=[];vec_list=[]
    for t in range(T+1):
        e_value,e_vec=np.linalg.eig(gt_file[t])
        # to make sure the eigenvectors are set for each row but not column.
        for c in range(len(e_vec)):
            e_vec[c]=e_vec[c].T
        value_list.append(e_value);vec_list.append(e_vec)
    return value_list,vec_list


def cluster_check(data_ns,tau,plot=True,unsteady_check=False):
    """
    
    sort clusters by their lifespans, store their birth date and death date.
    
    Parameters
    ----------
    data_ns : numpy.ndarray
        the no splitted data generated by file_conversion.
    tau : int
        the global relaxation time.
    plot : bool, optional
        If true, make plots. The default is True.
    unsteady_check : bool, optional
        If true, just plot the unsteady clusters. The default is False.

    Returns
    -------
    general_info : numpy.ndarray
        numpy.ndarray with format general_info[c][(c_index,t_start,t_end)]. 
        c: cluster c_index: index of cluster t_start/end: birth/death time.

    """
    start_info=[];end_info=[] # record t_start and t_end
    # initially all clusters start at tau
    for c in np.unique(data_ns[tau][:,3]):
        # debugging: print([c,tau])
        start_info.append([c,tau])
    
    # check the rare cases where a cluster is born at tau but immediately dead.
    c_start_check=np.unique(data_ns[tau+1][:,3])
    for c in np.unique(data_ns[tau][:,3]):
        if len(np.where(c_start_check==c)[0])==0:
            t_end=tau
            end_info.append([c,t_end])
    
    for t in range(tau+1,T): 
        c_previous=np.unique(data_ns[t-1][:,3]) # previous information
        c_current=np.unique(data_ns[t][:,3]) # current information
        c_next=np.unique(data_ns[t+1][:,3]) # the information for the next timestep
        # for every cluster at time t:
        for c in c_current:
            # len(np.where(c_next==c)[0])==0 if there is no c in c_next, and 1 if there is
            if len(np.where(c_previous==c)[0])==0:
                t_start=t
                # debugging: print([c,t_start])
                start_info.append([c,t_start])
            if len(np.where(c_next==c)[0])==0:
                t_end=t
                # debugging: print([c,t_end])
                end_info.append([c,t_end])
  
    # check the rare cases where a cluster is born at the end and dead immediately.    
    c_end_check=np.unique(data_ns[T-1][:,3])
    for c in np.unique(data_ns[T][:,3]):
        if len(np.where(c_end_check==c)[0])==0: 
            t_start=T
            start_info.append([c,t_start])
  
    # finally all clusters end at T
    for c in np.unique(data_ns[T][:,3]):
        end_info.append([c,T])
        
    # set them to be np.arrays so that [:,0] is applicable
    start_info=np.array(start_info);end_info=np.array(end_info)
    general_info=[] # the combination of starting and ending info
    
    # for any cluster in start_info, it must end in end_info, and the order is kept
    # i.e. start_info and end_info have the same length
    while len(start_info)>0:
        # print(len(start_info)) debugging
        # each time we just consider the first cluster (earliest) in start_info
        c_check,t_check_start=start_info[0]
        # the earliest possible indices in end_info
        i_earliest=np.where(end_info[:,0]==c_check)[0][0]
        t_check_end=end_info[i_earliest,1]
        general_info.append([c_check,t_check_start,t_check_end])
        # after finishing one check we delete the line used in start_info and end_info
        start_info=np.delete(start_info,0,0)
        end_info=np.delete(end_info,i_earliest,0)
        
    general_info=np.array(general_info)
    
    if plot:
        # plot 
        plt.figure(dpi=300)
        for c in range(len(general_info)):
            c_index=general_info[c,0]
            x_start=general_info[c,1]; x_end=general_info[c,2] # starting and ending time
            x_plot=np.linspace(x_start,x_end,100)
            y_plot=np.linspace(c_index,c_index,100)
            # open the unsteady check, so only unsteady clusters appear:
            if unsteady_check:
                if x_start!=tau or x_end!=T:
                    plt.plot(x_plot,y_plot)
            # else all clusters are plotted
            else:
                plt.plot(x_plot,y_plot)       
        plt.xlabel(r'$t$')
        plt.ylabel(r'index of cluster (in LAMMPS)')
        plt.title('lifespan of clusters')
        plt.show()
    
    return general_info


def max_dist(points): 
    """

    Find the longest distance between two points in a dataset. DATE: 29/11/2022

    Parameters
    ----------
    points : numpy.ndarray
        The positions of the dataset.

    Returns
    -------
    dist_max : numpy.float64
        The longest distance among all points in the dataset.

    """        
    # the convex hull of the dataset
    hull = ConvexHull(points)
    hullpoints = points[hull.vertices,:] 
    # Naive way of finding the best pair in O(H^2) time if H is number of points on hull     
    hdist = cdist(hullpoints, hullpoints, metric='euclidean')
    # Get the farthest apart points
    bestpair = np.unravel_index(hdist.argmax(), hdist.shape)
    # the longest distance
    dist_max=np.linalg.norm(hullpoints[bestpair[0]]-hullpoints[bestpair[1]])
    return dist_max
